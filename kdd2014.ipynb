{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data...\n",
      "complete..\n",
      "(619326, 12)\n",
      "(664098, 46)\n",
      "(3667217, 9)\n",
      "(664098, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:175: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:221: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:244: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:247: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:251: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:252: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/hang/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:254: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# imports\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier,     RandomForestRegressor, RandomForestClassifier\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# start to calculate history rate for different attributes \n",
    "id_columns = ['teacher_acctid','schoolid']\n",
    "location_columns = ['school_city','school_state','school_district','school_county','school_zip']\n",
    "all_columns = id_columns + location_columns\n",
    "\n",
    "requirements_columns = ['projectid','date_posted','is_exciting','at_least_1_teacher_referred_donor','fully_funded','at_least_1_green_donation',\n",
    "                        'great_chat','one_or_more_required']\n",
    "donation_columns = ['is_teacher_acct','donation_total']\n",
    "require_only_columns = ['is_exciting','at_least_1_teacher_referred_donor','fully_funded','at_least_1_green_donation','great_chat','one_or_more_required']\n",
    "\n",
    "\n",
    "def calculate_possibility(df, title, columns=require_only_columns):\n",
    "    df_p = df \n",
    "    alpha = 10\n",
    "    for var in require_only_columns:\n",
    "        mean = df_p[var].sum()/df_p['count'].sum()\n",
    "        #print(mean)\n",
    "        df_p[var] = (df_p[var]+mean*alpha)/(df_p['count']+alpha)\n",
    "        df_p[var][df_p['count']==1] = 0      \n",
    "        df_p.columns = df_p.columns.str.replace(var,\"\".join(title+'_'+var+'_rate'))\n",
    "    df_p.rename(columns={'count': \"\".join(title+'_count')}, inplace=True)\n",
    "    return df_p\n",
    "def calculate_possibility_by_time(df,title,start_date,end_date,columns= require_only_columns):\n",
    "    df_p = df[(df['date_posted']<end_date)&(df['date_posted']>=start_date)]\n",
    "    alpha = 10\n",
    "    for var in require_only_columns:\n",
    "        mean = df_p[var].sum()/df_p['count'].sum()\n",
    "        #print(mean)\n",
    "        df_p[var] = (df_p[var]+mean*alpha)/(df_p['count']+alpha)\n",
    "        df_p[var][df_p['count']==1] = 0      \n",
    "        df_p.columns = df_p.columns.str.replace(var,\"\".join(title+'_'+var))\n",
    "    df_p.rename(columns={'count': \"\".join(title+'_count')}, inplace=True)\n",
    "    return df_p\n",
    "\n",
    "def calculate_sum(df,name):\n",
    "    #df_tmp = df[df['date_posted']<\"2014-01-01\"]\n",
    "    df_tmp = df \n",
    "    name_group = df_tmp.groupby(name,as_index=False)\n",
    "    count = pd.DataFrame({'count':name_group.size()}).reset_index()\n",
    "    prev = name_group.sum()\n",
    "    prev = pd.merge(prev,count,on=name)\n",
    "    df_tmp = pd.merge(df_tmp[['projectid',name]],prev,how='left',on=name)\n",
    "    return df_tmp\n",
    "\n",
    "def calculate_by_time(df,name,start_date,end_date):\n",
    "    df_tmp = df[(df['date_posted']<end_date)&(df['date_posted']>=start_date)]\n",
    "    name_group = df_tmp.groupby(name,as_index=False)\n",
    "    count = pd.DataFrame({'count':name_group.size()}).reset_index()\n",
    "    prev = name_group.sum()\n",
    "    prev = pd.merge(prev,count,on=name)\n",
    "    df_tmp = pd.merge(df_tmp[['projectid','date_posted',name]],prev,how='left',on=name)\n",
    "    df_tmp = df_tmp.drop('date_posted',axis=1)\n",
    "    return df_tmp\n",
    "\n",
    "def get_full_df():\n",
    "    test_df = df\n",
    "    #data = df[(df['cat']=='train')|(df['cat']=='val')]\n",
    "    for var in all_columns:\n",
    "        #get columns of requirements \n",
    "        name_tmp = df[np.append(requirements_columns,var)]\n",
    "        # calculate sum accroding to different attribute\n",
    "        sum_tmp = calculate_sum(name_tmp,var)\n",
    "        #calculate the possibility of different attribute\n",
    "        pos_tmp = calculate_possibility(sum_tmp,var)\n",
    "        # merge the result to the main df dataframe\n",
    "        test_df = pd.merge(test_df,pos_tmp,how='left',on=['projectid',var])\n",
    "        print(\"finish merge for \"+ var)\n",
    "    for var in donation_columns:\n",
    "        name_tp = df[np.append(donation_columns,var)]\n",
    "        sum_tmp = calculate_sum(name_tmp,var)\n",
    "        pos_tmp = calculate_possibility(sum_tmp,var)\n",
    "        test_df = pd.merge(test_df,pos_tmp,how='left',on=['projectid',var])\n",
    "        print(\"finish merge for \"+ var)\n",
    "    return test_df\n",
    "\n",
    "\n",
    "# this function is not finished yet\n",
    "def get_date_partition(start_date, end_date, interval):\n",
    "    intervals = []\n",
    "    start_time = pd.to_datetime(start_date)\n",
    "    end_time = pd.to_datetime(end_date)\n",
    "    bet = end_time - start_time\n",
    "    times = bet/interval\n",
    "    for i in range(0,times):\n",
    "        end = start_time + interval\n",
    "        start_time.dt.strftime()\n",
    "\n",
    "\n",
    "\n",
    "print('loading the data...')\n",
    "projects = pd.read_csv('./data/projects.csv')\n",
    "outcomes = pd.read_csv('./data/outcomes.csv')\n",
    "print('complete..')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "print(outcomes.shape)\n",
    "outcomes = outcomes.sort_values(by='projectid')\n",
    "outcomes.fillna(method='pad')\n",
    "outcomes.head()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "projects = projects.sort_values(by='projectid')\n",
    "projects.fillna(method='pad')\n",
    "projects.head()\n",
    "df = pd.merge(projects, outcomes, how='left', on='projectid')\n",
    "print(df.shape)\n",
    "df[df['is_exciting'].isnull()].shape  # all the test is_exciting value is null \n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "resources = pd.read_csv('./data/resources.csv')\n",
    "print(resources.shape)\n",
    "resources['cost'] = resources['item_unit_price']*resources['item_quantity']\n",
    "#resources.head()\n",
    "total_sum = resources[['projectid','cost','item_quantity']].groupby('projectid',as_index=False).aggregate(np.sum)\n",
    "total_sum['avg_cost'] = total_sum['cost']/total_sum['item_quantity']\n",
    "total_sum.head()\n",
    "df = pd.merge(df, total_sum, how='left', on='projectid')\n",
    "\n",
    "for var in resources['project_resource_type'].unique()[0:6]:\n",
    "    tmp = resources[(resources['project_resource_type']==var)]\n",
    "    tmp_sum= tmp[['projectid','cost']].groupby('projectid',as_index=False).aggregate(np.sum)\n",
    "    tmp_sum.rename(columns={'cost': \"\".join(var+'_cost')}, inplace=True)\n",
    "    df = pd.merge(df,tmp_sum,how='left',on ='projectid')\n",
    "    df[\"\".join(var+'_cost')].fillna(0)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "df['price_per_student'] = df['cost']/df['students_reached']\n",
    "\n",
    "essays = pd.read_csv('./data/essays.csv')\n",
    "essays.fillna(\"\",inplace=True)\n",
    "essays['essay_length'] = essays['essay'].str.len()\n",
    "essays['title_length'] = essays['title'].str.len()\n",
    "df = pd.merge(df,essays[['projectid','essay_length','title_length']],on='projectid',how='left')\n",
    "\n",
    "\n",
    "#add category for the data \n",
    "df['cat'] = \"train\"\n",
    "df['cat'][df[\"date_posted\"]<\"2010-01-01\"] = \"nouse\"\n",
    "# valadation set\n",
    "df['cat'][df[\"date_posted\"]>=\"2013-01-01\"] = \"val\"\n",
    "# test set\n",
    "df['cat'][df[\"date_posted\"]>=\"2014-01-01\"]= \"test\"\n",
    "df = df[df['cat']!=\"nouse\"]\n",
    "\n",
    "df['is_exciting'][df['is_exciting']==\"t\"] = 1\n",
    "df['is_exciting'][df['is_exciting']==\"f\"] = 0\n",
    "df['is_exciting'].fillna(0,inplace=True)\n",
    "\n",
    "df[\"at_least_1_teacher_referred_donor\"][df[\"at_least_1_teacher_referred_donor\"]==\"t\"] = 1\n",
    "df[\"at_least_1_teacher_referred_donor\"][df[\"at_least_1_teacher_referred_donor\"]==\"f\"] = 0\n",
    "df[\"at_least_1_teacher_referred_donor\"].fillna(0,inplace=True)\n",
    "\n",
    "df[\"great_chat\"][df[\"great_chat\"]==\"t\"] = 1\n",
    "df[\"great_chat\"][df[\"great_chat\"]==\"f\"] = 0\n",
    "df[\"great_chat\"].fillna(0,inplace=True)\n",
    "\n",
    "df[\"fully_funded\"][df[\"fully_funded\"]==\"t\"] = 1\n",
    "df[\"fully_funded\"][df[\"fully_funded\"]==\"f\"] = 0\n",
    "df[\"fully_funded\"].fillna(0,inplace=True)\n",
    "\n",
    "df[\"at_least_1_green_donation\"][df[\"at_least_1_green_donation\"]==\"t\"] = 1\n",
    "df[\"at_least_1_green_donation\"][df[\"at_least_1_green_donation\"]==\"f\"] = 0\n",
    "df[\"at_least_1_green_donation\"].fillna(0,inplace=True)\n",
    "\n",
    "df[\"donation_from_thoughtful_donor\"][df[\"donation_from_thoughtful_donor\"]==\"t\"] = 1\n",
    "df[\"donation_from_thoughtful_donor\"][df[\"donation_from_thoughtful_donor\"]==\"f\"] = 0\n",
    "df[\"donation_from_thoughtful_donor\"].fillna(0,inplace=True)\n",
    "\n",
    "df[\"three_or_more_non_teacher_referred_donors\"][df[\"three_or_more_non_teacher_referred_donors\"]==\"t\"] = 1\n",
    "df[\"three_or_more_non_teacher_referred_donors\"][df[\"three_or_more_non_teacher_referred_donors\"]==\"f\"] = 0\n",
    "df[\"three_or_more_non_teacher_referred_donors\"].fillna(0,inplace=True)\n",
    "\n",
    "df[\"one_non_teacher_referred_donor_giving_100_plus\"][df[\"one_non_teacher_referred_donor_giving_100_plus\"]==\"t\"] = 1\n",
    "df[\"one_non_teacher_referred_donor_giving_100_plus\"][df[\"one_non_teacher_referred_donor_giving_100_plus\"]==\"f\"] = 0\n",
    "df[\"one_non_teacher_referred_donor_giving_100_plus\"].fillna(0,inplace=True)\n",
    "\n",
    "df['teacher_referred_count'][df['teacher_referred_count']<1] = 0\n",
    "df['teacher_referred_count'][df['teacher_referred_count']>=1] = 1\n",
    "df['teacher_referred_count'].fillna(0,inplace=True)\n",
    "\n",
    "df['non_teacher_referred_count'][df['non_teacher_referred_count']<1] = 0\n",
    "df['non_teacher_referred_count'][df['non_teacher_referred_count']>=3] = 1\n",
    "df['non_teacher_referred_count'].fillna(0,inplace=True)\n",
    "\n",
    "one_or_more_required = ['three_or_more_non_teacher_referred_donors','one_non_teacher_referred_donor_giving_100_plus',\n",
    "                       'donation_from_thoughtful_donor']\n",
    "df['one_or_more_required'] = df[one_or_more_required].sum(axis=1)\n",
    "df['one_or_more_required'][df['one_or_more_required']>=1] = 1\n",
    "df['one_or_more_required'][df['one_or_more_required']<1] = 0\n",
    "# this was indicated in the great_chat no need to cal again \n",
    "#df['great_messages_proportion'].fillna(0,inplace=True)\n",
    "#df['great_messages_proportion'][df['great_messages_proportion'] >= 62] = True\n",
    "#df['great_messages_proportion'][df['great_messages_proportion'] < 62] = False\n",
    "#df['great_messages_proportion'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "\n",
    "# add time tag columns for the data, maybe used for calculate history features\n",
    "df[\"year\"] = df[\"date_posted\"].apply(lambda x: x.split(\"-\")[0])\n",
    "df[\"month\"] = df[\"date_posted\"].apply(lambda x: x.split(\"-\")[1])\n",
    "df[\"day\"] = df[\"date_posted\"].apply(lambda x: x.split(\"-\")[2])\n",
    "# convert time to int and sort by time\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['month'] = df['month'].astype(int)\n",
    "df['day'] = df['day'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "df['school_ncesid'] = df['school_ncesid'].apply(str)\n",
    "df['school_zip'] = df['school_zip'].apply(str)\n",
    "\n",
    "df['grade_level'][df['grade_level']==\"Grades PreK-2\"] = 0.0\n",
    "df['grade_level'][df['grade_level']==\"Grades 3-5\"] = 1.0\n",
    "df['grade_level'][df['grade_level']==\"Grades 6-8\"] = 2.0\n",
    "df['grade_level'][df['grade_level']==\"Grades 9-12\"] = 3.0\n",
    "\n",
    "df['poverty_level'][df['poverty_level']=='highest poverty'] = 3.0\n",
    "df['poverty_level'][df['poverty_level']=='high poverty'] = 2.0\n",
    "df['poverty_level'][df['poverty_level']=='moderate poverty'] = 1.0\n",
    "df['poverty_level'][df['poverty_level']=='low poverty'] = 0.0\n",
    "\n",
    "df.sort('date_posted',inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "df['date_posted'] = pd.to_datetime(df['date_posted'])\n",
    "ref_date = \"2010-01-01\"\n",
    "ref_date = pd.to_datetime(ref_date)\n",
    "print(ref_date)\n",
    "df['daysbet'] = df['date_posted'] - ref_date\n",
    "df['daysbet'] = df['daysbet'].dt.days\n",
    "df['monthbet'] = df['date_posted'] - ref_date\n",
    "df['monthbet'] = df['monthbet'].dt.days\n",
    "df['monthbet'] = df['monthbet']/30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# merge donation information\n",
    "donations = pd.read_csv('./data/donations.csv')\n",
    "donation_df = pd.merge(projects,donations,how='left',on='projectid')\n",
    "donation_df = donation_df[['projectid','teacher_acctid','schoolid','is_teacher_acct','donation_total']]\n",
    "donation_df[\"is_teacher_acct\"][donation_df[\"is_teacher_acct\"]==\"t\"] = 1\n",
    "donation_df[\"is_teacher_acct\"][donation_df[\"is_teacher_acct\"]==\"f\"] = 0\n",
    "donation_df[\"is_teacher_acct\"].fillna(0,inplace=True)\n",
    "donation_df[\"donation_total\"].fillna(0,inplace=True)\n",
    "#donation_df2=donation_df.groupby(by=[\"projectid\",\"date_posted\",\"teacher_acctid\",\"schoolid\",\"school_district\",\"school_city\",\"school_zip\"])[[\"count\",\"is_teacher_acct\",\"donation_total\"]].sum()\n",
    "#donation_df2.reset_index(inplace=True)\n",
    "\n",
    "donor_sum = calculate_sum(donation_df,'teacher_acctid')\n",
    "donor_sum['tis_teacher_acct'] = donor_sum['is_teacher_acct']/donor_sum['count']\n",
    "donor_sum['donation_total'] = donor_sum['donation_total']/donor_sum['count']\n",
    "donor_sum.columns.str.replace('is_teacher_acct','teacher_acctid_is_teacher_acct')\n",
    "donor_sum.columns.str.replace('donation_total','teacher_acctid_donation_total')\n",
    "df = pd.merge(df,donor_sum,how='left',on=['projectid','teacher_acctid'])\n",
    "\n",
    "\n",
    "donor_sum_2 = calculate_sum(donation_df,'schoolid')\n",
    "donor_sum_2['is_teacher_acct'] = donor_sum_2['is_teacher_acct']/donor_sum_2['count']\n",
    "donor_sum_2['donation_total'] = donor_sum_2['donation_total']/donor_sum_2['count']\n",
    "donor_sum_2.columns.str.replace('is_teacher_acct','schoolid_is_teacher_acct')\n",
    "donor_sum_2.columns.str.replace('donation_total','schoolid_donation_total')\n",
    "df = pd.merge(df,donor_sum_2,how='left',on=['projectid','schoolid'])\n",
    "\n",
    "df.fillna(method='pad',inplace=True)\n",
    "\n",
    "\n",
    "example = get_full_df()\n",
    "example.head()\n",
    "#data = df[(df['cat']=='train')|(df['cat']=='val')]\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "date_columns = ['day','month','year']\n",
    "feature_columns = example.columns - example.columns[0:4] - requirements_columns-['cat'] - one_or_more_required - sub_primary_requirements - ['day']\n",
    "\n",
    "example.fillna(method='pad',inplace=True)\n",
    "\n",
    "example.to_csv('origin_data.csv',index=False)\n",
    "\n",
    "\n",
    "sub_primary_requirements = ['great_messages_proportion','teacher_referred_count','non_teacher_referred_count'] \n",
    "projects_numeric_columns = ['school_latitude','school_longitude','fulfillment_labor_materials',\n",
    "                           'total_price_excluding_optional_support',\n",
    "                           'total_price_including_optional_support','grade_level','poverty_level','students_reached']\n",
    "projects_id_columns =['projectid','teacher_acctid','schoolid','school_ncesid']\n",
    "projects_categorial_columns = projects.columns - projects_numeric_columns - projects_id_columns - ['date_posted']\n",
    "print(projects_categorial_columns)\n",
    "# the way to encode the category may be wrong\n",
    "\n",
    "for var in projects_categorial_columns:\n",
    "    le = LabelEncoder()\n",
    "    example[var] = le.fit_transform(example[var])\n",
    "example.tail()\n",
    "\n",
    "\n",
    "\n",
    "# after get full df, do feature selection \n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "train = example[(example['cat']=='train')|(df['cat']=='val')]\n",
    "data = train[feature_columns]\n",
    "X = data\n",
    "names = data.columns.values\n",
    "Y = train['is_exciting']\n",
    "rfr = RandomForestRegressor()\n",
    "print(\"start to fit the data\")\n",
    "rfr.fit(X,Y)\n",
    "print(\"finish fitting\")\n",
    "print( \"Features sorted by their score:\")\n",
    "feature_list = sorted(zip(map(lambda x: round(x, 4), rfr.feature_importances_), names), \n",
    "             reverse=True)\n",
    "print(feature_list)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "dctr = DecisionTreeRegressor()\n",
    "print(\"start to fit the data\")\n",
    "dctr.fit(X,Y)\n",
    "print(\"finish fitting\")\n",
    "print( \"Features sorted by their score:\")\n",
    "feature_list_2 = sorted(zip(map(lambda x: round(x, 4), dctr.feature_importances_), names), \n",
    "             reverse=True)\n",
    "print(feature_list_2)\n",
    "\n",
    "\n",
    "# after feature selection, select to drop some columns\n",
    "update_feature_columns = []\n",
    "for (v,n) in feature_list:\n",
    "    update_feature_columns.append(n)\n",
    "\n",
    "\n",
    "update_feature_columns_2 = []\n",
    "for (v,n) in feature_list_2:\n",
    "    update_feature_columns_2.append(n)\n",
    "\n",
    "\n",
    "used_feature = list(set(update_feature_columns[0:50]).union(set(update_feature_columns_2[0:50])))\n",
    "\n",
    "\n",
    "for_use_columns = ['projectid','is_exciting','cat'] + used_feature\n",
    "example[for_use_columns].to_csv('experiment_data.csv',index=False)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "# load data \n",
    "train = example[example['cat']=='train']\n",
    "val = example[example['cat']=='val']\n",
    "train_val = example[example['cat']!='test']\n",
    "test = example[example['cat']=='test']\n",
    "\n",
    "X_train = train[used_feature]\n",
    "X_val = val[used_feature]\n",
    "X_train_val = train_val[used_feature]\n",
    "X_test = test[used_feature]\n",
    "\n",
    "Y_train = train['is_exciting']\n",
    "Y_val = val['is_exciting']\n",
    "Y_train_val = train_val['is_exciting']\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n",
    "\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 300, 'max_depth': 7, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls','verbose':1,}\n",
    "clf = GradientBoostingRegressor(**params)\n",
    "print (\"start training\")\n",
    "clf.fit(X_train_val, Y_train_val)\n",
    "print(\"finish training\")\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "print(\"start to predict val\")\n",
    "predict = clf.predict(X_val)\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "predict[predict>0.5] = 1\n",
    "predict[predict<0.5] = 0\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(Y_val.values, predict)\n",
    "mse\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_val.values,predict)\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "test = example[example['cat']=='test']\n",
    "X_test = test[used_feature]\n",
    "X_test.shape\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "print(\"start to predict test value\")\n",
    "test_predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "test_predict[test_predict>0.5] = 1\n",
    "test_predict[test_predict<0.5] = 0\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "test_predict[test_predict==1].shape\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "test_predict[test_predict==0].shape\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "sample = pd.read_csv('./data/sampleSubmission.csv')\n",
    "sample['is_exciting'] = test_predict.astype(int)\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "sample.head()\n",
    "sample.to_csv('predictions.csv', index = False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
